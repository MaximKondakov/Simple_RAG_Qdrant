{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\badwh\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from llama_cpp import Llama\n",
    "import torch\n",
    "import numpy as np\n",
    "from spacy.lang.ru import Russian\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_name):\n",
    "    with open(file_name, encoding=\"utf-8\") as r:\n",
    "        return [json.loads(line) for line in r]\n",
    "\n",
    "\n",
    "def write_jsonl(records, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as w:\n",
    "        for r in records:\n",
    "            w.write(json.dumps(r, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number, character count, word count, and the extracted text for each page.\n",
    "    \"\"\"    \n",
    "    # Define the regular expression pattern to match a hyphen at the end of a line\n",
    "    pattern = r'-\\n|\\xad\\s'\n",
    "    # Use re.sub to replace the hyphen with an empty string and concatenate the next line\n",
    "    processed_text = re.sub(pattern, '', text)\n",
    "    processed_text = processed_text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def longest_common_substring(str1: str, \n",
    "                             str2: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds the longest common substring between two input strings.\n",
    "\n",
    "    Parameters:\n",
    "        str1 (str): The first input string.\n",
    "        str2 (str): The second input string.\n",
    "\n",
    "    Returns:\n",
    "        The longest common substring found in both input strings.\n",
    "    \"\"\"\n",
    "    # Create a 2D array to store lengths of longest common suffixes of substrings\n",
    "    m, n = len(str1), len(str2)\n",
    "    lcsuff = [[0] * (n + 1) for i in range(m + 1)]\n",
    "    length = 0  # To store length of the longest common substring\n",
    "    end_pos = 0  # To store end position of the longest common substring in str1\n",
    "\n",
    "    # Building the lcsuff table in bottom-up fashion\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                lcsuff[i][j] = 0\n",
    "            elif str1[i - 1] == str2[j - 1]:\n",
    "                lcsuff[i][j] = lcsuff[i - 1][j - 1] + 1\n",
    "                if lcsuff[i][j] > length:\n",
    "                    length = lcsuff[i][j]\n",
    "                    end_pos = i\n",
    "            else:\n",
    "                lcsuff[i][j] = 0\n",
    "\n",
    "    # The longest common substring is from end_pos - length to end_pos in str1\n",
    "    return str1[end_pos - length: end_pos]\n",
    "\n",
    "\n",
    "def longest_substring_dict(doc:object,\n",
    "                           introduction: int=0,\n",
    "                           number_pages: int=15) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the longest common substrings from a document and counts their occurrences.\n",
    "\n",
    "    This function processes a given document by extracting text from a specified\n",
    "    range of pages and finds the longest common substrings between consecutive pages.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    doc (object): The document object from which text is extracted.\n",
    "\n",
    "    introduction (int, optional): The starting page number from which text extraction\n",
    "                                  begins. Default is 0.\n",
    "    number_pages (int, optional): The number of pages to process from the starting\n",
    "                                  page. Default is 15.        \n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the longest common substrings found between\n",
    "              consecutive pages, and values are their counts of occurrences.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for page_number, page in tqdm(enumerate(doc[introduction : number_pages])): \n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        examples.append(text[:100])\n",
    "\n",
    "    longest_common = examples[0]\n",
    "    longest_common_dict = {}\n",
    "    for i in range(1, len(examples)):\n",
    "        longest_common = longest_common_substring(examples[i-1], examples[i])\n",
    "        if longest_common_dict.get(longest_common):\n",
    "            longest_common_dict[longest_common] += 1\n",
    "        else:\n",
    "            longest_common_dict[longest_common] = 1\n",
    "\n",
    "    return longest_common_dict\n",
    "\n",
    "\n",
    "def intro_and_conlusion_page(doc: object,\n",
    "                             search_first: str,\n",
    "                             search_second: str):\n",
    "    \"\"\"\n",
    "    Finds the page numbers for the \"Introduction\" and \"Conclusion\" sections in a document.\n",
    "\n",
    "    This function iterates through the pages of a document\n",
    "\n",
    "    Parameters:\n",
    "        doc (object): The document object from which text is extracted.\n",
    "\n",
    "        search_first (str): The term to search for in the document to identify the \"Introduction\"\n",
    "                            section.\n",
    "\n",
    "        search_second (str): The term to search for in the document to identify the \"Conclusion\"\n",
    "                            section.\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "           - The page number where the \"Introduction\" section starts.\n",
    "           - The page number where the \"Conclusion\" section starts.     \n",
    "    \"\"\"    \n",
    "    # Iterate through the pages to find the \"Introduction\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text().lower()  \n",
    "        if search_first in text:\n",
    "            introduction = page_num \n",
    "            break\n",
    "\n",
    "    # Iterate through the pages to find the \"Conclusion\"\n",
    "    for page_num in range(0, -len(doc), -1):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text().lower()  \n",
    "        if search_second in text:\n",
    "            conlusion = page_num \n",
    "            break\n",
    "\n",
    "    return introduction, conlusion\n",
    "\n",
    "\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF document, identifies and extracts text between the \"Introduction\" and \"Bibliography\" sections,\n",
    "    removes specific recurring text patterns, and returns a list of dictionaries containing page details.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Opens the specified PDF document.\n",
    "    2. Identifies the pages where the \"Introduction\" and \"Bibliography\" sections are located.\n",
    "    3. Finds the most common recurring text pattern at the beginning of each page (assumed to be a standard header or footer).\n",
    "    4. Removes this recurring pattern from the text of each page.\n",
    "    5. Extracts and formats the text from each page between the \"Introduction\" and \"Bibliography\" sections.\n",
    "    6. Returns a list of dictionaries, each containing the page number, character count, word count, and the cleaned text for each page.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_path (str): The file path to the PDF document to be processed.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, each containing the following keys:\n",
    "                - \"page_number\" (int): The page number in the extracted range.\n",
    "                - \"page_char_count\" (int): The number of characters on the page after cleaning.\n",
    "                - \"page_word_count\" (int): The number of words on the page after cleaning.\n",
    "                - \"text\" (str): The cleaned text content of the page.\n",
    "    \"\"\"\n",
    "    \n",
    "    # open a document\n",
    "    search_first = \"введение\".lower()\n",
    "    search_second = \"библиография\".lower()\n",
    "    introduction = 0\n",
    "    conlusion = -1\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    # добавить try except на наличие введения и библиографии, иначе ничего не обрезать\n",
    "    introduction, conlusion = intro_and_conlusion_page(doc, search_first, search_second)\n",
    "    longest_common_dict = longest_substring_dict(doc, introduction)\n",
    "    first_name_gost_on_every_page = max(longest_common_dict, key=longest_common_dict.get)\n",
    "\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc[introduction : conlusion])):  # iterate the document pages\n",
    "        text = page.get_text()  \n",
    "        text = text_formatter(text)\n",
    "        text = re.sub(first_name_gost_on_every_page, '', text, count=1) # remove first GOST on every page\n",
    "        pages_and_texts.append({\"page_number\": page_number,  \n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(input_list: list,\n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "\n",
    "def spacy_preprocessing_chunk(pages_and_texts: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Processes and chunks the text from pages using spaCy for sentence segmentation, and organizes the text into manageable chunks.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Uses spaCy to segment the text of each page into sentences.\n",
    "    2. Counts the number of sentences on each page.\n",
    "    3. Computes an average sentence chunk size based on the mean number of sentences per page.\n",
    "    4. Splits the sentences into chunks of the computed size.\n",
    "    5. Creates a list of dictionaries, each containing details of these chunks, while filtering out chunks with fewer than a specified number of words.\n",
    "\n",
    "    Parameters:\n",
    "    pages_and_texts (list[dict]): A list of dictionaries, where each dictionary contains:\n",
    "                                  - \"page_number\" (int): The page number in the extracted range.\n",
    "                                  - \"page_char_count\" (int): The number of characters on the page after cleaning.\n",
    "                                  - \"page_word_count\" (int): The number of words on the page after cleaning.\n",
    "                                  - \"text\" (str): The cleaned text content of the page.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, each containing:\n",
    "                - \"page_number\" (int): The page number from which the chunk was extracted.\n",
    "                - \"sentence_chunk\" (str): The text content of the chunk.\n",
    "                - \"chunk_char_count\" (int): The number of characters in the chunk.\n",
    "                - \"chunk_word_count\" (int): The number of words in the chunk.\n",
    "                \n",
    "    The function also adds the following keys to each input dictionary in `pages_and_texts`:\n",
    "                - \"sentences\" (list[str]): The sentences in the page's text.\n",
    "                - \"page_sentence_count_spacy\" (int): The number of sentences in the page's text.\n",
    "                - \"sentence_chunks\" (list[list[str]]): The text split into chunks of sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp_simple = Russian()\n",
    "    nlp_simple.add_pipe('sentencizer')\n",
    "\n",
    "    # how many sentence in page\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentences\"] = list(nlp_simple(item[\"text\"]).sents)\n",
    "        # Make sure all sentences are strings\n",
    "        item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "        # Count the sentences\n",
    "        item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n",
    "    num_sentence_chunk_size = np.round(sum(d['page_sentence_count_spacy'] for d in pages_and_texts) \n",
    "                                       / len(pages_and_texts)).astype(int)# size chunk based on mean sentences on the page\n",
    "\n",
    "    # Chunking our sentences together\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                            slice_size=num_sentence_chunk_size)\n",
    "\n",
    "    #Split each chunk into its own item\n",
    "    pages_and_chunks = []\n",
    "    min_word_count = 20 # min word should be in chunk\n",
    "\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "            # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([А-Я])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
    "            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk.lower()\n",
    "\n",
    "            # Get stats about the chunk\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "            \n",
    "            # remove chunk, which lesser min_word_count\n",
    "            if chunk_dict[\"chunk_word_count\"] < min_word_count:\n",
    "                continue\n",
    "            else:\n",
    "                pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "    return pages_and_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation dataset for finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_yagpt(context: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a prompt for YandexGPT to generate a dictionary of questions and answers based on provided context.\n",
    "\n",
    "    Parameters:\n",
    "            context (str): The context text based on which the questions and answers should be generated.\n",
    "\n",
    "    Returns:\n",
    "             dict: A dictionary formatted to be used as a prompt for YandexGPT, containing the model URI, completion options, and messages.\n",
    "    \"\"\"\n",
    "    promtpt_ya = {\n",
    "                \"modelUri\": \"gpt://b1gl4dfg2eqii24mp1fp/yandexgpt-lite\",\n",
    "                \"completionOptions\": {\n",
    "                    \"stream\": False,\n",
    "                    \"temperature\": 0.05,\n",
    "                    \"maxTokens\": \"2000\"\n",
    "                },\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                    \"role\": \"system\",\n",
    "                    \"text\": \"\"\"\n",
    "                    Задание: На основе предоставленного контекста, создай словарь  вопросов и ответов, которые содержат основную суть контекста.\n",
    "                      Каждый элемент словаря должен представлять собой пару: вопрос (ключ) и соответствующий ответ (значение).\n",
    "                    Формат ответа: { \"Вопрос\": \"string\", \"Ответ\": \"string\" }     \n",
    "                    \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"text\": f\"Контекст: {context}\"\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "    \n",
    "    return promtpt_ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ya_dataset(text_chunks: list[str],\n",
    "                      url: str, \n",
    "                      headers: dict) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generates a dataset of questions and answers from text chunks using YandexGPT.\n",
    "\n",
    "    This function takes a list of text chunks, sends each chunk to YandexGPT to generate\n",
    "    questions and answers, and compiles these into a list of dictionaries. Each dictionary\n",
    "    contains a question and its corresponding answer, along with a unique identifier.\n",
    "\n",
    "    Parameters:\n",
    "                text_chunks (list[str]): A list of text chunks to process.\n",
    "                url (str): The URL for the API endpoint.\n",
    "                headers (dict): A dictionary of headers to include in the API request.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, each containing:\n",
    "                - \"Вопрос\" (str): The generated question.\n",
    "                - \"Ответ\" (str): The corresponding answer.\n",
    "                - \"id\" (str): A unique identifier for the question-answer pair.\n",
    "    \n",
    "    The function filters out any question-answer pairs where the question is identical to the answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular expression to extract all words\n",
    "    pattern = r'\\{.*?\\}'\n",
    "    list_of_dict = []\n",
    "\n",
    "    for context in tqdm(text_chunks):\n",
    "        prompt = prompt_yagpt(context)\n",
    "        \n",
    "        json_prompt = json.dumps(prompt, indent = 4) # Convert Python to JSON  \n",
    "        result = requests.post(url, headers=headers, data=json_prompt)\n",
    "\n",
    "        if result.status_code == 429:\n",
    "            # Error Too Many Requests\n",
    "            time.sleep(5)\n",
    "            result = requests.post(url, headers=headers, data=json_prompt)\n",
    "        \n",
    "        question_answer = result.json()\n",
    "        json_question_answer = question_answer['result']['alternatives'][0]['message']['text']\n",
    "        dict_from_json = re.findall(pattern, json_question_answer, re.DOTALL)\n",
    "\n",
    "        for match in dict_from_json:\n",
    "            try:\n",
    "                json_dict = json.loads(match)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONDecodeError: {e}\")\n",
    "                continue\n",
    "\n",
    "            json_dict['id'] = str(hash(json_dict['Вопрос']))\n",
    "            list_of_dict.append(json_dict)\n",
    "\n",
    "    list_of_dict_no_dupl = [qa_dict  for qa_dict in list_of_dict if qa_dict['Вопрос'] != qa_dict['Ответ']] # clear llm bugs: answer == question\n",
    "\n",
    "    return list_of_dict_no_dupl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_ids(ya_dataset):\n",
    "    unique_data = {}\n",
    "    for item in ya_dataset:\n",
    "        unique_data[item['id']] = item\n",
    "        \n",
    "    unique_data_list = list(unique_data.values())\n",
    "\n",
    "    return unique_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 285.85it/s]\n",
      "38it [00:00, 363.08it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 250.46it/s]\n",
      "100%|██████████| 38/38 [00:00<?, ?it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 15137.58it/s]\n",
      " 55%|█████▌    | 31/56 [02:00<01:43,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ',' delimiter: line 4 column 3 (char 149)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [03:53<00:00,  4.17s/it]\n",
      "12it [00:00, 283.72it/s]\n",
      "51it [00:00, 349.92it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 247.52it/s]\n",
      "100%|██████████| 51/51 [00:00<?, ?it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 25510.97it/s]\n",
      "100%|██████████| 76/76 [05:00<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ',' delimiter: line 2 column 28 (char 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 422.05it/s]\n",
      "26it [00:00, 472.69it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 318.54it/s]\n",
      "100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 25983.30it/s]\n",
      " 62%|██████▏   | 21/34 [01:18<00:59,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [02:33<00:00,  4.51s/it]\n",
      "10it [00:00, 322.12it/s]\n",
      "30it [00:00, 467.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 217.22it/s]\n",
      "100%|██████████| 30/30 [00:00<?, ?it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 14992.15it/s]\n",
      "  5%|▍         | 2/44 [00:07<02:34,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Invalid control character at: line 3 column 42 (char 117)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [02:41<00:00,  3.68s/it]\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\{.*?\\}'\n",
    "list_of_dict = []\n",
    "\n",
    "url = 'https://llm.api.cloud.yandex.net/foundationModels/v1/completion'\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    'Authorization': f'Api-Key AQVN1WIE8wCWyEUClNkkAvo5djWbHmxuD6h27kxu'}\n",
    "\n",
    "PATH_DIR = \"D:\\\\my_project\"\n",
    "data_documents = os.listdir(PATH_DIR)\n",
    "filtered_data_documents = list(filter(lambda path: path.endswith('.pdf'), data_documents))\n",
    "path_to_filtered_data_documents  = list(map(lambda x: os.path.join(PATH_DIR, x), filtered_data_documents))\n",
    "\n",
    "ya_dataset = []\n",
    "for pdf_path in path_to_filtered_data_documents:\n",
    "    pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "    pages_and_chunks_over_min_word_len = spacy_preprocessing_chunk(pages_and_texts)\n",
    "    text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_word_len]\n",
    "    list_of_dict_no_dupl = create_ya_dataset(text_chunks, url, headers)\n",
    "    ya_dataset.extend(list_of_dict_no_dupl)\n",
    "\n",
    "\n",
    "result_ya_dataset = remove_duplicates_ids(ya_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_gpt_style_dataset(list_of_dict_no_dupl: list[dict]) -> Dataset:\n",
    "    \"\"\"\n",
    "    Converts a list of question-answer dictionaries into a GPT-style dataset format.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_dict_no_dupl (list[dict]): A list of dictionaries, each containing:\n",
    "                                       - \"Вопрос\" (str): The question.\n",
    "                                       - \"Ответ\" (str): The answer.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A Hugging Face Dataset object containing the formatted question-answer pairs.\n",
    "                Each entry in the dataset is a conversation with the following structure:\n",
    "                - \"messages\" (list[dict]): A list of messages in the conversation, each represented as a dictionary with:\n",
    "                                            - \"role\" (str): The role of the speaker (\"user\" or \"assistant\").\n",
    "                                            - \"content\" (str): The content of the message (the question or the answer).\n",
    "    \"\"\"\n",
    "    q_a_res = []\n",
    "    for q_a_pair in list_of_dict_no_dupl:\n",
    "        question = q_a_pair['Вопрос']\n",
    "        answer = q_a_pair['Ответ']\n",
    "        chat1 = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}]\n",
    "        q_a_res.append(chat1)\n",
    "\n",
    "    dataset_raw = Dataset.from_dict({\"messages\": q_a_res})\n",
    "    \n",
    "    return dataset_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = convert_to_gpt_style_dataset(result_ya_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(dataset_raw, \"ya_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = read_jsonl(\"ya_dataset.json\")\n",
    "dataset_raw = Dataset.from_list(dataset_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "client = QdrantClient(url=QDRANT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\my_project\\\\4293773880.pdf',\n",
       " 'D:\\\\my_project\\\\4293774215.pdf',\n",
       " 'D:\\\\my_project\\\\51213.pdf',\n",
       " 'D:\\\\my_project\\\\56236.pdf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_DIR = \"D:\\\\my_project\"\n",
    "data_documents = os.listdir(PATH_DIR)\n",
    "filtered_data_documents = list(filter(lambda path: path.endswith('.pdf'), data_documents))\n",
    "path_to_filtered_data_documents  = list(map(lambda x: os.path.join(PATH_DIR, x), filtered_data_documents))\n",
    "path_to_filtered_data_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from D:\\my_project\\llama_8b_gost.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = model\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128000\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = model\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4403.49 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'model', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "925"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga_llama3_8b\")\n",
    "# model_name = \"model-q2_K\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:\\\\my_project\\\\lora_adapter\")\n",
    "model_name = \"llama_8b_gost.Q4_K_M\"\n",
    "model_path=f\"D:\\\\my_project\\\\{model_name}.gguf\"\n",
    "\n",
    "model_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    verbose=True,\n",
    "    n_ctx=4048,\n",
    "    n_gpu_layers=-1\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\badwh\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\badwh\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# embedding_model for the sentence\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "embedding_model = embedding_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_collection(path_to_filtered_data_documents: list[str],\n",
    "                                size_embedding: int,\n",
    "                                collection_name: str = \"gost_document_embeddings\",\n",
    "                                 ):\n",
    "    \"\"\"\n",
    "    Creates and uploads embeddings for PDF documents to a specified collection.\n",
    "\n",
    "    This function processes a list of PDF documents, extracts text, preprocesses the text using spaCy,\n",
    "    generates embeddings for text chunks, and uploads these embeddings to a collection.\n",
    "\n",
    "    Parameters:\n",
    "            path_to_filtered_data_documents (list[str]): A list of file paths to the PDF documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "            None: The function uploads embeddings to the collection and does not return any value.\n",
    "    \"\"\"\n",
    "\n",
    "    if client.collection_exists(collection_name=collection_name):\n",
    "        print(\"collection already exist\")\n",
    "        \n",
    "    else:\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=size_embedding, distance=Distance.DOT),\n",
    "        )\n",
    "\n",
    "\n",
    "    for pdf_path in (path_to_filtered_data_documents):\n",
    "        \n",
    "        pages_and_texts = open_and_read_pdf(pdf_path = pdf_path)\n",
    "        pages_and_chunks_over_min_word_len = spacy_preprocessing_chunk(pages_and_texts)\n",
    "\n",
    "        text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_word_len]\n",
    "\n",
    "        text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                        batch_size=32, # you can use different batch sizes here for speed/performance\n",
    "                                        convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "        \n",
    "        # Create multiple point structures\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                    id = uuid.uuid4().hex,\n",
    "                    vector = embedding_vector.tolist(),\n",
    "                    payload = {\n",
    "                        'document_name': os.path.basename(pdf_path),\n",
    "                        'page_number': pages_and_chunks_over_min_word_len[idx]['page_number'],\n",
    "                        'sentence_chunk': pages_and_chunks_over_min_word_len[idx][\"sentence_chunk\"]\n",
    "                        }\n",
    "                )\n",
    "                for idx, embedding_vector in enumerate(text_chunk_embeddings)]\n",
    "            \n",
    "        client.upload_points(\n",
    "                            collection_name,\n",
    "                            points=points,\n",
    "                            batch_size=64,\n",
    "                            )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection already exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 289.96it/s]\n",
      "38it [00:00, 376.78it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 247.26it/s]\n",
      "100%|██████████| 38/38 [00:00<?, ?it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 22464.21it/s]\n",
      "12it [00:00, 266.72it/s]\n",
      "51it [00:00, 353.63it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 254.89it/s]\n",
      "100%|██████████| 51/51 [00:00<?, ?it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 25501.85it/s]\n",
      "11it [00:00, 464.39it/s]\n",
      "26it [00:00, 485.59it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 315.11it/s]\n",
      "100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "10it [00:00, 391.83it/s]\n",
      "30it [00:00, 490.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 226.03it/s]\n",
      "100%|██████████| 30/30 [00:00<?, ?it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 11976.88it/s]\n"
     ]
    }
   ],
   "source": [
    "size_embedding = embedding_model._modules['1'].word_embedding_dimension\n",
    "\n",
    "create_embedding_collection(path_to_filtered_data_documents, size_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter_qdrant(query: str,\n",
    "                            context_items: list ) -> str:\n",
    "    \"\"\"\n",
    "    Formats a query and context items into a prompt for a language model.\n",
    "\n",
    "    This function takes a query and a list of context items, formats them into a structured prompt, \n",
    "    and applies a chat template for use with a language model.\n",
    "\n",
    "    Parameters:\n",
    "            query (str): The question or query to be answered.\n",
    "            context_items (list): A list of context items, where each item has a payload containing a \"sentence_chunk\".\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted prompt string for use with a language model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item.payload[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "    Контекст: \n",
    "    {context}\\n\\n\n",
    "    Используя контекст, ответь на вопрос: {query}\"\"\"\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_answer_llm_qdrant(query: str,\n",
    "                           collection_name: str = \"gost_document_embeddings\"):\n",
    "    \"\"\"\n",
    "    Generates an answer to a query using context from multiple collections.\n",
    "\n",
    "    Parameters:\n",
    "            query (str): The query or question for which an answer is sought.\n",
    "\n",
    "    Returns:\n",
    "            str: The generated answer from the language model based on the provided query and context.\n",
    "    \"\"\"\n",
    "    search_result = client.search(\n",
    "                                collection_name=collection_name,\n",
    "                                query_vector=embedding_model.encode(query),\n",
    "                                limit=5\n",
    "                            )\n",
    "    \n",
    "    print('Relevant information')\n",
    "    for i in search_result:\n",
    "        print(i)\n",
    "    # Format prompt with context items\n",
    "    prompt = prompt_formatter_qdrant(query=query,\n",
    "                                    context_items=search_result)\n",
    "\n",
    "    output_text = model_llm(prompt, \n",
    "                        max_tokens=1000, \n",
    "                        stop=model_llm.token_eos(), \n",
    "                        )\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant information\n",
      "id='d02d3ea0-fa3c-42c9-a715-58eb03e557d5' version=4 score=11.713477 payload={'document_name': '56236.pdf', 'page_number': 4, 'sentence_chunk': '3.30 облучение (irradiation): воздействие излучения на материалы или живые существа. в радиологии — воздействие на живой организм или материал ионизирующим излучением, например. рентгеновское облучение.3.31 выключатель облучения (irradiation switch): в радиологическом изделии — устройство управления, обеспечивающее начало и/или прекращение облучения.3.32 время облучения (irradiation time): продолжительность облучения, определяемая по специальным методам; обычно время, в течение которого радиационная величина превышает определенный уровень.3.33 излучение утечки (leakage radiation): ионизирующее излучение, прошедшее через защитный экран источника излучения, а также ионизирующее излучение, которое в рентгеновских генераторах некоторых типов выходит через родиационное окно перед нагрузкой и после (например, в аппарате с рентгеновской трубкой, имеющей управляющую сетку).3.34 нагрузка (loading): в рентгеновском генераторе — приложение питания электрической энергией к аноду рентгеновской трубки.4'} vector=None shard_key=None\n",
      "id='c5b870ea-57de-4747-8024-5b2e3c7bb97c' version=4 score=9.424991 payload={'document_name': '56236.pdf', 'page_number': 7, 'sentence_chunk': '3.61 источник излучения (radiation source): часть изделия, способного испускать ионизирующее излучение.3.62 блок источника излучения (radiation source assembly): блок, содержащий в сборе: - источник излучения; - средства, обеспечивающие защиту от ионизирующего излучения и.если необходимо, электробезопасность: - систему формирования пучка. пример — блок источника рентгеновского излучения.3.63 паспортные условия рентгенографии (radiographic rating): применительно к работе рентгеновской трубки — нормированное сочетание условий и параметров нагрузки, при которых достигается нормированный предел нагрузочной способности рентгеновской трубки.3.64 рентгенография (radiography): методика получения, записи и управления обработкой.непосредственно или после передачи. информации, содержащейся в рентгеновском изображении на поверхности приемника изображения, с целью анализа в течение промежутка времени, не зависящего от времени облучения.3.65 радиологический (radiological): определение, относящееся к ионизирующему излучению, его генерированию и применению в научных, медицинских и технических целях.3.66 радиологическое изображение (radiological image): информация, получаемая с использованием ионизирующего излучения, представленная в виде изображения, удобного для медицинской диагностики.3.67 радиологическая установка (radiological installation): установленное радиологическое изделие, включая все средства для его работы. п ример — рентгеновская установка.3.68 радиология (radiology): наука об ионизирующем излучении и его использовании.3.69 рентгеноскопия (radioscopy): методика получения непрерывного или ряда периодических рентгеновских изображений и представления их одновременно и непрерывно как видимых изображений непосредственно или после передачи изображения и его обработки с целью обеспечения руководства для дальнейших действий в реальном масштабе времени.3.70 опорное значение воздушной кермы (reference air kerma): воздушная керма.освобожденная в воздухе в первичном пучке рентгеновского излучения, измеренная при нормированных условиях и отнесенная к входной опорной точке пациента.3.71 опорное значение мощности воздушной кермы (reference air kerma rate): мощность воздушной кермы.освобожденная в воздухе в первичном пучке рентгеновского излучения, измеренная при нормированных условиях и отнесенная к входной опорной точке пациента.'} vector=None shard_key=None\n",
      "id='621bd72c-5aab-4c0c-ad27-0753c4b78100' version=4 score=9.227831 payload={'document_name': '56236.pdf', 'page_number': 7, 'sentence_chunk': '3.72 остаточное излучение (residual radiation): в медицинской радиологии та часть пучка излучения, которая остается после прохождения поверхности приемника изображения и соответствующего измерительного радиационного прибора.3.73 рассеянное излучение (scattered radiation): ионизирующее излучение, образовавшееся в результате взаимодействия ионизирующего излучения с веществом, сопровождаемого уменьшением энергии излучения и/или изменением направления излучения.3.74 особая зона пребывания (significant zone of occupancy): применительно к рентгеновскому аппарату — зона с точно определенными границами внутри наблюдаемой или контролируемой зоны, но не в защищенной зоне, обусловленная необходимостью пребывания в ней во время облучения.3.75 неиспользуемое излучение (stray radiation): для ионизирующего излучения — все излучение, за исключением рассматриваемого нормируемого пучка излучения, подлежащего использованию, но включая остаточное излучение.3.76 стационарная защита (structural shielding): защитное средство, являющееся частью конструкции помещения, в котором находится радиациологическая установка.3.77 общая фильтрация (total filtration): полная фильтрация, состоящая из собственной фильтрации и дополнительной фильтраций.3.78 рентгеновский аппарат (x-ray equipment): изделие, состоящее из рентгеновского генератора. вспомогательного оборудования и приспособлений.7'} vector=None shard_key=None\n",
      "id='397e68ae-6a3f-4b8e-808d-2c47f57535d3' version=1 score=9.023782 payload={'document_name': '4293774215.pdf', 'page_number': 42, 'sentence_chunk': 'размеры в сантиметрах рисунок 203.105 — испытание на неиспользуемое излучение (вертикальный пучок рентгеновского излучения при блоке источника рентгеновского излучения под штативом пациента) 42'} vector=None shard_key=None\n",
      "id='148a5e08-4c34-418a-b5a3-1705d9ccb1d6' version=4 score=8.944978 payload={'document_name': '56236.pdf', 'page_number': 12, 'sentence_chunk': 'п р и м е ч а н и е — использование значений а соответствии с этими геометрическими прогрессиями помогает оператору в настройке количества рентгеновского излучения, которое важно как с точки зрения дозы излучения на пациента, так и качества изображения. с оответствие проверяется экспертизой и функциональными испытаниями.6.3.2 воспроизводимость выходного излучения если в частном стандарте нет ссылки на эту тему, то для такого изделия в файле менеджмента риска должна быть определена воспроизводимость выходного излучения относительно установленных параметров нагрузки, которые требуются для предусмотренного применения. в эксплуатационных документах должна быть указана точность выходного излучения. с оответствие проверяется экспертизой.12'} vector=None shard_key=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     249.88 ms\n",
      "llama_print_timings:      sample time =      19.87 ms /    53 runs   (    0.37 ms per token,  2667.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =     835.40 ms /    53 runs   (   15.76 ms per token,    63.44 tokens per second)\n",
      "llama_print_timings:       total time =    1160.08 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Что такое качество излучения?\n",
      "RAG ответ: В контексте медицинской радиологии под термином \"качество излучения\"\n",
      "понимается соответствие рентгеновского излучения определенным параметрам\n",
      "нагрузки, обеспечивающим необходимое для медицинских целей изображение.\n"
     ]
    }
   ],
   "source": [
    "# Пример запроса\n",
    "query = \"Что такое качество излучения?\"\n",
    "\n",
    "output_text = give_answer_llm_qdrant(query)\n",
    "\n",
    "print(f\"Вопрос: {query}\")\n",
    "print_wrapped(f\"RAG ответ:\\n{output_text['choices'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
